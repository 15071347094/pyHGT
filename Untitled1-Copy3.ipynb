{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from data import *\n",
    "from utils import *\n",
    "from model import *\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.argv = ['fake']\n",
    "        \n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training GNN on Paper-Venue (Journal) classification task')\n",
    "\n",
    "'''\n",
    "    Dataset arguments\n",
    "'''\n",
    "parser.add_argument('--data_dir', type=str, default='./dataset/oag_output',\n",
    "                    help='The address of preprocessed graph.')\n",
    "parser.add_argument('--model_dir', type=str, default='./model_save',\n",
    "                    help='The address for storing the models and optimization results.')\n",
    "parser.add_argument('--task_name', type=str, default='PV',\n",
    "                    help='The name of the stored models and optimization results.')\n",
    "parser.add_argument('--cuda', type=int, default=0,\n",
    "                    help='Avaiable GPU ID')\n",
    "parser.add_argument('--domain', type=str, default='_CS',\n",
    "                    help='CS, Medicion or All: _CS or _Med or (empty)')         \n",
    "'''\n",
    "   Model arguments \n",
    "'''\n",
    "parser.add_argument('--conv_name', type=str, default='hgt',\n",
    "                    choices=['hgt', 'gcn', 'gat', 'rgcn', 'han', 'hetgnn'],\n",
    "                    help='The name of GNN filter. By default is Heterogeneous Graph Transformer (hgt)')\n",
    "parser.add_argument('--n_hid', type=int, default=400,\n",
    "                    help='Number of hidden dimension')\n",
    "parser.add_argument('--n_heads', type=int, default=8,\n",
    "                    help='Number of attention head')\n",
    "parser.add_argument('--n_layers', type=int, default=3,\n",
    "                    help='Number of GNN layers')\n",
    "parser.add_argument('--dropout', type=int, default=0.2,\n",
    "                    help='Dropout ratio')\n",
    "parser.add_argument('--sample_depth', type=int, default=6,\n",
    "                    help='How many numbers to sample the graph')\n",
    "parser.add_argument('--sample_width', type=int, default=128,\n",
    "                    help='How many nodes to be sampled per layer per type')\n",
    "\n",
    "'''\n",
    "    Optimization arguments\n",
    "'''\n",
    "parser.add_argument('--optimizer', type=str, default='adamw',\n",
    "                    choices=['adamw', 'adam', 'sgd', 'adagrad'],\n",
    "                    help='optimizer to use.')\n",
    "parser.add_argument('--data_percentage', type=int, default=1.0,\n",
    "                    help='Percentage of training and validation data to use')\n",
    "parser.add_argument('--n_epoch', type=int, default=200,\n",
    "                    help='Number of epoch to run')\n",
    "parser.add_argument('--n_pool', type=int, default=4,\n",
    "                    help='Number of process to sample subgraph')    \n",
    "parser.add_argument('--n_batch', type=int, default=32,\n",
    "                    help='Number of batch (sampled graphs) for each epoch')   \n",
    "parser.add_argument('--repeat', type=int, default=2,\n",
    "                    help='How many time to train over a singe batch (reuse data)') \n",
    "parser.add_argument('--batch_size', type=int, default=256,\n",
    "                    help='Number of output nodes for training')    \n",
    "parser.add_argument('--clip', type=int, default=0.25,\n",
    "                    help='Gradient Norm Clipping') \n",
    "\n",
    "\n",
    "args = parser.parse_args('--data_dir /datadrive/dataset --model_dir /datadrive/model_save'.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda != -1:\n",
    "    device = torch.device(\"cuda:2\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "graph = dill.load(open(os.path.join(args.data_dir, 'graph%s.pk' % args.domain), 'rb'))\n",
    "\n",
    "train_range = {t: True for t in graph.times if t != None and t < 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and t >= 2015  and t <= 2016}\n",
    "test_range  = {t: True for t in graph.times if t != None and t > 2016}\n",
    "\n",
    "types = graph.get_types()\n",
    "'''\n",
    "    cand_list stores all the Journal, which is the classification domain.\n",
    "'''\n",
    "cand_list = list(graph.edge_list['venue']['paper']['PV_Journal'].keys())\n",
    "'''\n",
    "Use CrossEntropy (log-softmax + NLL) here, since each paper can be associated with one venue.\n",
    "'''\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def node_classification_sample(seed, pairs, time_range, batch_size):\n",
    "    '''\n",
    "        sub-graph sampling and label preparation for node classification:\n",
    "        (1) Sample batch_size number of output nodes (papers)\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    target_ids = np.random.choice(list(pairs.keys()), batch_size, replace = False)\n",
    "    target_info = []\n",
    "    '''\n",
    "        (2) Get all the source_nodes (Journal) associated with these output nodes.\n",
    "            Collect their information and time as seed nodes for sampling sub-graph.\n",
    "    '''\n",
    "    for target_id in target_ids:\n",
    "        _, _time = pairs[target_id]\n",
    "        target_info += [[target_id, _time]]\n",
    "\n",
    "    '''\n",
    "        (3) Based on the seed nodes, sample a subgraph with 'sampled_depth' and 'sampled_number'\n",
    "    '''\n",
    "    feature, times, edge_list, _, _ = sample_subgraph(graph, time_range, \\\n",
    "                inp = {'paper': np.array(target_info)}, \\\n",
    "                sampled_depth = args.sample_depth, sampled_number = args.sample_width)\n",
    "\n",
    "\n",
    "    '''\n",
    "        (4) Mask out the edge between the output target nodes (paper) with output source nodes (Journal)\n",
    "    '''\n",
    "    masked_edge_list = []\n",
    "    for i in edge_list['paper']['venue']['rev_PV_Journal']:\n",
    "        if i[0] >= batch_size:\n",
    "            masked_edge_list += [i]\n",
    "    edge_list['paper']['venue']['rev_PV_Journal'] = masked_edge_list\n",
    "\n",
    "    masked_edge_list = []\n",
    "    for i in edge_list['venue']['paper']['PV_Journal']:\n",
    "        if i[1] >= batch_size:\n",
    "            masked_edge_list += [i]\n",
    "    edge_list['venue']['paper']['PV_Journal'] = masked_edge_list\n",
    "    \n",
    "    '''\n",
    "        (5) Transform the subgraph into torch Tensor (edge_index is in format of pytorch_geometric)\n",
    "    '''\n",
    "    node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = \\\n",
    "            to_torch(feature, times, edge_list, graph)\n",
    "    '''\n",
    "        (6) Prepare the labels for each output target node (paper), and their index in sampled graph.\n",
    "            (node_dict[type][0] stores the start index of a specific type of nodes)\n",
    "    '''\n",
    "    ylabel = torch.zeros(batch_size, dtype = torch.long)\n",
    "    for x_id, target_id in enumerate(target_ids):\n",
    "        ylabel[x_id] = cand_list.index(pairs[target_id][0])\n",
    "    x_ids = np.arange(batch_size) + node_dict['paper'][0]\n",
    "    return node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel\n",
    "    \n",
    "def prepare_data(pool):\n",
    "    '''\n",
    "        Sampled and prepare training and validation data using multi-process parallization.\n",
    "    '''\n",
    "    jobs = []\n",
    "    for batch_id in np.arange(args.n_batch):\n",
    "        p = pool.apply_async(node_classification_sample, args=(randint(), \\\n",
    "            sel_train_pairs, train_range, args.batch_size))\n",
    "        jobs.append(p)\n",
    "    p = pool.apply_async(node_classification_sample, args=(randint(), \\\n",
    "            sel_valid_pairs, valid_range, args.batch_size))\n",
    "    jobs.append(p)\n",
    "    return jobs\n",
    "\n",
    "\n",
    "train_pairs = {}\n",
    "valid_pairs = {}\n",
    "test_pairs  = {}\n",
    "'''\n",
    "    Prepare all the souce nodes (Journal) associated with each target node (paper) as dict\n",
    "'''\n",
    "for target_id in graph.edge_list['paper']['venue']['rev_PV_Journal']:\n",
    "    for source_id in graph.edge_list['paper']['venue']['rev_PV_Journal'][target_id]:\n",
    "        _time = graph.edge_list['paper']['venue']['rev_PV_Journal'][target_id][source_id]\n",
    "        if _time in train_range:\n",
    "            if target_id not in train_pairs:\n",
    "                train_pairs[target_id] = [source_id, _time]\n",
    "        elif _time in valid_range:\n",
    "            if target_id not in valid_pairs:\n",
    "                valid_pairs[target_id] = [source_id, _time]\n",
    "        else:\n",
    "            if target_id not in test_pairs:\n",
    "                test_pairs[target_id]  = [source_id, _time]\n",
    "\n",
    "\n",
    "np.random.seed(43)\n",
    "'''\n",
    "    Only train and valid with a certain percentage of data, if necessary.\n",
    "'''\n",
    "sel_train_pairs = {p : train_pairs[p] for p in np.random.choice(list(train_pairs.keys()), int(len(train_pairs) * args.data_percentage), replace = False)}\n",
    "sel_valid_pairs = {p : valid_pairs[p] for p in np.random.choice(list(valid_pairs.keys()), int(len(valid_pairs) * args.data_percentage), replace = False)}\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGTConv(MessagePassing):\n",
    "    def __init__(self, in_dim, out_dim, num_types, num_relations, n_heads, dropout = 0.2, **kwargs):\n",
    "        super(HGTConv, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_dim        = in_dim\n",
    "        self.out_dim       = out_dim\n",
    "        self.num_types     = num_types\n",
    "        self.num_relations = num_relations\n",
    "        self.total_rel     = num_types * num_relations * num_types\n",
    "        self.n_heads       = n_heads\n",
    "        self.d_k           = out_dim // n_heads\n",
    "        self.sqrt_dk       = math.sqrt(self.d_k)\n",
    "        self.att           = None\n",
    "        \n",
    "        self.k_linears   = nn.ModuleList()\n",
    "        self.q_linears   = nn.ModuleList()\n",
    "        self.v_linears   = nn.ModuleList()\n",
    "        self.a_linears   = nn.ModuleList()\n",
    "        \n",
    "        for t in range(num_types):\n",
    "            self.k_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.q_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.v_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.a_linears.append(nn.Linear(out_dim,  out_dim))\n",
    "            \n",
    "        '''\n",
    "            TODO: make relation_pri smaller, as not all <st, rt, tt> pair exist in meta relation list.\n",
    "        '''\n",
    "        self.relation_pri   = nn.Parameter(torch.ones(num_types, num_relations, num_types, self.n_heads))\n",
    "        self.relation_att   = nn.Parameter(torch.Tensor(num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.relation_msg   = nn.Parameter(torch.Tensor(num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.skip           = nn.Parameter(torch.ones(num_types))\n",
    "        self.drop           = nn.Dropout(dropout)\n",
    "        self.emb            = RelTemporalEncoding(in_dim)\n",
    "        \n",
    "        glorot(self.relation_att)\n",
    "        glorot(self.relation_msg)\n",
    "        \n",
    "    def forward(self, node_inp, node_type, edge_index, edge_type, edge_time):\n",
    "        return self.propagate(edge_index, node_inp=node_inp, node_type=node_type, \\\n",
    "                              edge_type=edge_type, edge_time = edge_time)\n",
    "\n",
    "    def message(self, edge_index_i, node_inp_i, node_inp_j, node_type_i, node_type_j, edge_type, edge_time, num_nodes):\n",
    "        '''\n",
    "            j: source, i: target; <j, i>\n",
    "        '''\n",
    "        data_size = edge_index_i.size(0)\n",
    "        '''\n",
    "            Create Attention and Message tensor beforehand.\n",
    "        '''\n",
    "        res_att     = torch.zeros(data_size, self.n_heads).to(node_inp_i.device)\n",
    "        res_msg     = torch.zeros(data_size, self.n_heads, self.d_k).to(node_inp_i.device)\n",
    "        \n",
    "        for source_type in range(self.num_types):\n",
    "            sb = (node_type_j == int(source_type))\n",
    "            k_linear = self.k_linears[source_type]\n",
    "            v_linear = self.v_linears[source_type] \n",
    "            for target_type in range(self.num_types):\n",
    "                tb = (node_type_i == int(target_type)) & sb\n",
    "                q_linear = self.q_linears[target_type]\n",
    "                for relation_type in range(self.num_relations):\n",
    "                    '''\n",
    "                        idx is all the edges with meta relation <source_type, relation_type, target_type>\n",
    "                    '''\n",
    "                    idx = (edge_type == int(relation_type)) & tb\n",
    "                    if idx.sum() == 0:\n",
    "                        continue\n",
    "                    '''\n",
    "                        Get the corresponding input node representations by idx.\n",
    "                        Add tempotal encoding to source representation (j)\n",
    "                    '''\n",
    "                    target_node_vec = node_inp_i[idx]\n",
    "                    source_node_vec = self.emb(node_inp_j[idx], edge_time[idx])\n",
    "\n",
    "                    '''\n",
    "                        Step 1: Heterogeneous Mutual Attention\n",
    "                    '''\n",
    "                    q_mat = q_linear(target_node_vec).view(-1, self.n_heads, self.d_k)\n",
    "                    k_mat = k_linear(source_node_vec).view(-1, self.n_heads, self.d_k)\n",
    "                    k_mat = torch.bmm(k_mat.transpose(1,0), self.relation_att[relation_type]).transpose(1,0)\n",
    "                    res_att[idx] = (q_mat * k_mat).sum(dim=-1) * \\\n",
    "                        self.relation_pri[target_type][relation_type][source_type] / self.sqrt_dk\n",
    "                    '''\n",
    "                        Step 2: Heterogeneous Message Passing\n",
    "                    '''\n",
    "                    v_mat = v_linear(source_node_vec).view(-1, self.n_heads, self.d_k)\n",
    "                    res_msg[idx] = torch.bmm(v_mat.transpose(1,0), self.relation_msg[relation_type]).transpose(1,0)   \n",
    "        '''\n",
    "            Softmax based on target node's id (edge_index_i). Store attention value in self.att for later visualization.\n",
    "        '''\n",
    "        self.att = softmax(res_att, edge_index_i, data_size)\n",
    "        res = res_msg * self.att.view(-1, self.n_heads, 1)\n",
    "        del res_att, res_msg\n",
    "        return res.view(-1, self.out_dim)\n",
    "\n",
    "\n",
    "    def update(self, aggr_out, node_inp, node_type):\n",
    "        '''\n",
    "            Step 3: Target-specific Aggregation\n",
    "            x = W[node_type] * gelu(Agg(x)) + x\n",
    "        '''\n",
    "        aggr_out = F.gelu(aggr_out)\n",
    "        res = torch.zeros(aggr_out.size(0), self.out_dim).to(node_inp.device)\n",
    "        for target_type in range(self.num_types):\n",
    "            idx = (node_type == int(target_type))\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            '''\n",
    "                Add skip connection with learnable weight self.skip[t_id]\n",
    "            '''\n",
    "            alpha = F.sigmoid(self.skip[target_type])\n",
    "            res[idx] = self.a_linears[target_type](aggr_out[idx]) * alpha + node_inp[idx] * (1 - alpha)\n",
    "        return self.drop(res)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(in_dim={}, out_dim={}, num_types={}, num_types={})'.format(\n",
    "            self.__class__.__name__, self.in_dim, self.out_dim,\n",
    "            self.num_types, self.num_relations)\n",
    "\n",
    "class RelTemporalEncoding(nn.Module):\n",
    "    '''\n",
    "        Implement the Temporal Encoding (Sinusoid) function.\n",
    "    '''\n",
    "    def __init__(self, n_hid, max_len = 240, dropout = 0.2):\n",
    "        super(RelTemporalEncoding, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = 1 / (10000 ** (torch.arange(0., n_hid * 2, 2.)) / n_hid / 2)\n",
    "        self.emb = nn.Embedding(max_len, n_hid * 2)\n",
    "        self.emb.weight.data[:, 0::2] = torch.sin(position * div_term) / math.sqrt(n_hid)\n",
    "        self.emb.weight.data[:, 1::2] = torch.cos(position * div_term) / math.sqrt(n_hid)\n",
    "        self.emb.requires_grad = False\n",
    "        self.lin = nn.Linear(n_hid * 2, n_hid)\n",
    "    def forward(self, x, t):\n",
    "        return x + self.drop(self.lin(self.emb(t)))\n",
    "    \n",
    "    \n",
    "    \n",
    "class GeneralConv(nn.Module):\n",
    "    def __init__(self, conv_name, in_hid, out_hid, num_types, num_relations, n_heads, dropout):\n",
    "        super(GeneralConv, self).__init__()\n",
    "        self.conv_name = conv_name\n",
    "        if self.conv_name == 'hgt':\n",
    "            self.base_conv = HGTConv(in_hid, out_hid, num_types, num_relations, n_heads, dropout)\n",
    "        elif self.conv_name == 'gcn':\n",
    "            self.base_conv = GCNConv(in_hid, out_hid)\n",
    "        elif self.conv_name == 'gat':\n",
    "            self.base_conv = GATConv(in_hid, out_hid // n_heads, heads=n_heads)\n",
    "    def forward(self, meta_xs, node_type, edge_index, edge_type, edge_time):\n",
    "        if self.conv_name == 'hgt':\n",
    "            return self.base_conv(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        elif self.conv_name == 'gcn':\n",
    "            return self.base_conv(meta_xs, edge_index)\n",
    "        elif self.conv_name == 'gat':\n",
    "            return self.base_conv(meta_xs, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_dim, n_hid, num_types, num_relations, n_heads, n_layers, dropout = 0.2, conv_name = 'hgt'):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.num_types = num_types\n",
    "        self.in_dim    = in_dim\n",
    "        self.n_hid     = n_hid\n",
    "        self.adapt_ws  = nn.ModuleList()\n",
    "        self.drop      = nn.Dropout(dropout)\n",
    "        for t in range(num_types):\n",
    "            self.adapt_ws.append(nn.Linear(in_dim, n_hid))\n",
    "        for l in range(n_layers):\n",
    "            self.gcs.append(HGTConv(n_hid, n_hid, num_types, num_relations, n_heads, dropout))\n",
    "\n",
    "    def forward(self, node_feature, node_type, edge_time, edge_index, edge_type):\n",
    "        res = torch.zeros(node_feature.size(0), self.n_hid).to(node_feature.device)\n",
    "        for t_id in range(self.num_types):\n",
    "            idx = (node_type == int(t_id))\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            res[idx] = torch.tanh(self.adapt_ws[t_id](node_feature[idx]))\n",
    "        meta_xs = self.drop(res)\n",
    "        del res\n",
    "        for gc in self.gcs:\n",
    "            meta_xs = gc(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        return meta_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Initialize GNN (model is specified by conv_name) and Classifier\n",
    "'''\n",
    "gnn = GNN(conv_name = args.conv_name, in_dim = len(graph.node_feature['paper']['emb'][0]) + 401, \\\n",
    "          n_hid = args.n_hid, n_heads = args.n_heads, n_layers = args.n_layers, dropout = args.dropout,\\\n",
    "          num_types = len(graph.get_types()), num_relations = len(graph.get_meta_graph()) + 1).to(device)\n",
    "classifier = Classifier(args.n_hid, len(cand_list)).to(device)\n",
    "\n",
    "model = nn.Sequential(gnn, classifier)\n",
    "\n",
    "\n",
    "if args.optimizer == 'adamw':\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "elif args.optimizer == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "elif args.optimizer == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "elif args.optimizer == 'adagrad':\n",
    "    optimizer = torch.optim.Adagrad(model.parameters())\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000, eta_min=1e-6)\n",
    "\n",
    "stats = []\n",
    "res = []\n",
    "best_val   = 0\n",
    "train_step = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 170.2s\n",
      "UPDATE!!!\n",
      "Epoch: 1 (200.3s)  LR: 0.00060 Train Loss: 6.78  Valid Loss: 6.75  Valid NDCG: 0.1890  Norm: 336.8959\n",
      "Data Preparation: 3.1s\n",
      "UPDATE!!!\n",
      "Epoch: 2 (196.9s)  LR: 0.00070 Train Loss: 6.13  Valid Loss: 6.48  Valid NDCG: 0.2300  Norm: 353.1761\n",
      "Data Preparation: 3.2s\n",
      "UPDATE!!!\n",
      "Epoch: 3 (202.9s)  LR: 0.00078 Train Loss: 5.70  Valid Loss: 5.89  Valid NDCG: 0.2718  Norm: 469.7213\n",
      "Data Preparation: 3.4s\n",
      "UPDATE!!!\n",
      "Epoch: 4 (179.5s)  LR: 0.00086 Train Loss: 5.49  Valid Loss: 5.85  Valid NDCG: 0.3081  Norm: 525.3987\n",
      "Data Preparation: 3.8s\n",
      "Epoch: 5 (198.0s)  LR: 0.00092 Train Loss: 5.33  Valid Loss: 5.82  Valid NDCG: 0.3002  Norm: 456.4521\n",
      "Data Preparation: 4.1s\n",
      "UPDATE!!!\n",
      "Epoch: 6 (198.7s)  LR: 0.00097 Train Loss: 5.17  Valid Loss: 5.64  Valid NDCG: 0.3147  Norm: 538.4780\n",
      "Data Preparation: 3.5s\n",
      "UPDATE!!!\n",
      "Epoch: 7 (192.0s)  LR: 0.00099 Train Loss: 4.98  Valid Loss: 5.33  Valid NDCG: 0.3674  Norm: 519.6523\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 8 (191.0s)  LR: 0.00100 Train Loss: 4.93  Valid Loss: 5.36  Valid NDCG: 0.3520  Norm: 501.6701\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 9 (193.8s)  LR: 0.00099 Train Loss: 4.84  Valid Loss: 5.54  Valid NDCG: 0.3317  Norm: 508.3852\n",
      "Data Preparation: 3.7s\n",
      "Epoch: 10 (201.0s)  LR: 0.00095 Train Loss: 4.69  Valid Loss: 5.48  Valid NDCG: 0.3542  Norm: 552.1454\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 11 (197.9s)  LR: 0.00090 Train Loss: 4.62  Valid Loss: 5.23  Valid NDCG: 0.3402  Norm: 559.2439\n",
      "Data Preparation: 3.5s\n",
      "UPDATE!!!\n",
      "Epoch: 12 (202.6s)  LR: 0.00083 Train Loss: 4.51  Valid Loss: 4.86  Valid NDCG: 0.4214  Norm: 552.9678\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 13 (199.7s)  LR: 0.00075 Train Loss: 4.38  Valid Loss: 5.52  Valid NDCG: 0.3365  Norm: 509.5217\n",
      "Data Preparation: 4.1s\n",
      "UPDATE!!!\n",
      "Epoch: 14 (201.0s)  LR: 0.00066 Train Loss: 4.40  Valid Loss: 4.94  Valid NDCG: 0.4220  Norm: 559.3519\n",
      "Data Preparation: 4.0s\n",
      "UPDATE!!!\n",
      "Epoch: 15 (200.3s)  LR: 0.00056 Train Loss: 4.31  Valid Loss: 5.09  Valid NDCG: 0.4264  Norm: 554.1904\n",
      "Data Preparation: 3.5s\n",
      "UPDATE!!!\n",
      "Epoch: 16 (201.2s)  LR: 0.00046 Train Loss: 4.25  Valid Loss: 4.83  Valid NDCG: 0.4299  Norm: 543.3512\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 17 (206.5s)  LR: 0.00036 Train Loss: 4.13  Valid Loss: 5.08  Valid NDCG: 0.3856  Norm: 565.4589\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 18 (196.9s)  LR: 0.00027 Train Loss: 4.15  Valid Loss: 4.95  Valid NDCG: 0.4005  Norm: 528.1918\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 19 (200.9s)  LR: 0.00019 Train Loss: 4.04  Valid Loss: 4.69  Valid NDCG: 0.4120  Norm: 539.1204\n",
      "Data Preparation: 3.6s\n",
      "Epoch: 20 (200.8s)  LR: 0.00012 Train Loss: 4.07  Valid Loss: 4.71  Valid NDCG: 0.4183  Norm: 540.8056\n",
      "Data Preparation: 3.8s\n",
      "UPDATE!!!\n",
      "Epoch: 21 (205.1s)  LR: 0.00006 Train Loss: 4.00  Valid Loss: 4.52  Valid NDCG: 0.4599  Norm: 530.9929\n",
      "Data Preparation: 4.0s\n",
      "Epoch: 22 (204.7s)  LR: 0.00002 Train Loss: 4.03  Valid Loss: 4.69  Valid NDCG: 0.4309  Norm: 525.4695\n",
      "Data Preparation: 3.8s\n",
      "Epoch: 23 (198.3s)  LR: 0.00000 Train Loss: 4.01  Valid Loss: 4.78  Valid NDCG: 0.4400  Norm: 524.0737\n",
      "Data Preparation: 4.3s\n",
      "Epoch: 24 (201.4s)  LR: 0.00000 Train Loss: 4.07  Valid Loss: 4.69  Valid NDCG: 0.4353  Norm: 529.1063\n",
      "Data Preparation: 4.2s\n",
      "Epoch: 25 (204.5s)  LR: 0.00003 Train Loss: 4.00  Valid Loss: 4.41  Valid NDCG: 0.4543  Norm: 522.7989\n",
      "Data Preparation: 3.8s\n",
      "Epoch: 26 (203.8s)  LR: 0.00007 Train Loss: 3.97  Valid Loss: 4.71  Valid NDCG: 0.4183  Norm: 520.2348\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 27 (199.8s)  LR: 0.00012 Train Loss: 3.98  Valid Loss: 4.54  Valid NDCG: 0.4438  Norm: 538.5619\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 28 (198.5s)  LR: 0.00020 Train Loss: 3.95  Valid Loss: 4.93  Valid NDCG: 0.4160  Norm: 548.2665\n",
      "Data Preparation: 3.9s\n",
      "UPDATE!!!\n",
      "Epoch: 29 (196.4s)  LR: 0.00028 Train Loss: 3.88  Valid Loss: 4.45  Valid NDCG: 0.4667  Norm: 544.3765\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 30 (205.5s)  LR: 0.00038 Train Loss: 3.92  Valid Loss: 4.55  Valid NDCG: 0.4391  Norm: 571.0849\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 31 (205.8s)  LR: 0.00048 Train Loss: 3.89  Valid Loss: 4.90  Valid NDCG: 0.4170  Norm: 557.8296\n",
      "Data Preparation: 3.3s\n",
      "UPDATE!!!\n",
      "Epoch: 32 (201.2s)  LR: 0.00058 Train Loss: 3.88  Valid Loss: 4.30  Valid NDCG: 0.4914  Norm: 606.0782\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 33 (200.5s)  LR: 0.00067 Train Loss: 3.86  Valid Loss: 4.44  Valid NDCG: 0.4634  Norm: 602.7548\n",
      "Data Preparation: 3.4s\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(args.n_pool)\n",
    "st = time.time()\n",
    "jobs = prepare_data(pool)\n",
    "\n",
    "for epoch in np.arange(args.n_epoch) + 1:\n",
    "    '''\n",
    "        Prepare Training and Validation Data\n",
    "    '''\n",
    "    train_data = [job.get() for job in jobs[:-1]]\n",
    "    valid_data = jobs[-1].get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    '''\n",
    "        After the data is collected, close the pool and then reopen it.\n",
    "    '''\n",
    "    pool = mp.Pool(args.n_pool)\n",
    "    jobs = prepare_data(pool)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    '''\n",
    "        Train (time < 2015)\n",
    "    '''\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    torch.cuda.empty_cache()\n",
    "    for _ in range(args.repeat):\n",
    "        for node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel in train_data:\n",
    "            node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            res  = classifier.forward(node_rep[x_ids])\n",
    "            loss = criterion(res, ylabel.to(device))\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            torch.cuda.empty_cache()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses += [loss.cpu().detach().tolist()]\n",
    "            train_step += 1\n",
    "            scheduler.step(train_step)\n",
    "            del res, loss\n",
    "    '''\n",
    "        Valid (2015 <= time <= 2016)\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel = valid_data\n",
    "        node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "        res  = classifier.forward(node_rep[x_ids])\n",
    "        loss = criterion(res, ylabel.to(device))\n",
    "        \n",
    "        '''\n",
    "            Calculate Valid NDCG. Update the best model based on highest NDCG score.\n",
    "        '''\n",
    "        valid_res = []\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            valid_res += [(bi == ai).int().tolist()]\n",
    "        valid_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in valid_res])\n",
    "        \n",
    "        if valid_ndcg > best_val:\n",
    "            best_val = valid_ndcg\n",
    "            torch.save(model, os.path.join(args.model_dir, args.task_name + '_abc'))\n",
    "            print('UPDATE!!!')\n",
    "        \n",
    "        st = time.time()\n",
    "        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %.2f  Valid Loss: %.2f  Valid NDCG: %.4f  Norm: %.4f\") % \\\n",
    "              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), \\\n",
    "                    loss.cpu().detach().tolist(), valid_ndcg, node_rep.norm().detach().tolist()))\n",
    "        stats += [[np.average(train_losses), loss.cpu().detach().tolist()]]\n",
    "        del res, loss\n",
    "    del train_data, valid_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 144.8s\n",
      "Epoch: 165 (154.3s)  LR: 0.00100 Train Loss: 3.01  Valid Loss: 3.68  Valid NDCG: 0.5564  Norm: 519.2357\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 166 (149.5s)  LR: 0.00099 Train Loss: 3.01  Valid Loss: 4.03  Valid NDCG: 0.5341  Norm: 497.2152\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 167 (156.5s)  LR: 0.00096 Train Loss: 2.96  Valid Loss: 4.06  Valid NDCG: 0.5299  Norm: 511.2950\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 168 (147.4s)  LR: 0.00092 Train Loss: 3.00  Valid Loss: 4.29  Valid NDCG: 0.5327  Norm: 517.9756\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 169 (146.6s)  LR: 0.00085 Train Loss: 2.97  Valid Loss: 4.36  Valid NDCG: 0.4995  Norm: 501.4537\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 170 (147.0s)  LR: 0.00077 Train Loss: 2.96  Valid Loss: 4.16  Valid NDCG: 0.4941  Norm: 499.8823\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 171 (147.3s)  LR: 0.00068 Train Loss: 2.99  Valid Loss: 4.29  Valid NDCG: 0.5168  Norm: 484.8323\n",
      "Data Preparation: 3.0s\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(args.n_pool)\n",
    "st = time.time()\n",
    "jobs = prepare_data(pool)\n",
    "\n",
    "for epoch in np.arange(args.n_epoch - epoch) + 1 + epoch:\n",
    "    '''\n",
    "        Prepare Training and Validation Data\n",
    "    '''\n",
    "    train_data = [job.get() for job in jobs[:-1]]\n",
    "    valid_data = jobs[-1].get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    '''\n",
    "        After the data is collected, close the pool and then reopen it.\n",
    "    '''\n",
    "    pool = mp.Pool(args.n_pool)\n",
    "    jobs = prepare_data(pool)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    '''\n",
    "        Train (time < 2015)\n",
    "    '''\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    torch.cuda.empty_cache()\n",
    "    for _ in range(args.repeat):\n",
    "        for node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel in train_data:\n",
    "            node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            res  = classifier.forward(node_rep[x_ids])\n",
    "            loss = criterion(res, ylabel.to(device))\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            torch.cuda.empty_cache()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses += [loss.cpu().detach().tolist()]\n",
    "            train_step += 1\n",
    "            scheduler.step(train_step)\n",
    "            del res, loss\n",
    "    '''\n",
    "        Valid (2015 <= time <= 2016)\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel = valid_data\n",
    "        node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "        res  = classifier.forward(node_rep[x_ids])\n",
    "        loss = criterion(res, ylabel.to(device))\n",
    "        \n",
    "        '''\n",
    "            Calculate Valid NDCG. Update the best model based on highest NDCG score.\n",
    "        '''\n",
    "        valid_res = []\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            valid_res += [(bi == ai).int().tolist()]\n",
    "        valid_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in valid_res])\n",
    "        \n",
    "        if valid_ndcg > best_val:\n",
    "            best_val = valid_ndcg\n",
    "            torch.save(model, os.path.join(args.model_dir, args.task_name + '_abc'))\n",
    "            print('UPDATE!!!')\n",
    "        \n",
    "        st = time.time()\n",
    "        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %.2f  Valid Loss: %.2f  Valid NDCG: %.4f  Norm: %.4f\") % \\\n",
    "              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), \\\n",
    "                    loss.cpu().detach().tolist(), valid_ndcg, node_rep.norm().detach().tolist()))\n",
    "        stats += [[np.average(train_losses), loss.cpu().detach().tolist()]]\n",
    "        del res, loss\n",
    "    del train_data, valid_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_res = []\n",
    "    for _ in range(10):\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel = \\\n",
    "                    node_classification_sample(randint(), test_pairs, test_range, args.batch_size)\n",
    "        paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                    edge_time.to(device), edge_index.to(device), edge_type.to(device))[x_ids]\n",
    "        res = classifier.forward(paper_rep)\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            test_res += [(bi == ai).int().tolist()]\n",
    "    test_ndcg = [ndcg_at_k(resi, len(resi)) for resi in test_res]\n",
    "    print('Last Test NDCG: %.4f' % np.average(test_ndcg))\n",
    "    test_mrr = mean_reciprocal_rank(test_res)\n",
    "    print('Last Test MRR:  %.4f' % np.average(test_mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(os.path.join(args.model_dir, args.task_name + '_abc'))\n",
    "best_model.eval()\n",
    "gnn, classifier = best_model\n",
    "with torch.no_grad():\n",
    "    test_res = []\n",
    "    for _ in range(10):\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel = \\\n",
    "                    node_classification_sample(randint(), test_pairs, test_range, args.batch_size)\n",
    "        paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                    edge_time.to(device), edge_index.to(device), edge_type.to(device))[x_ids]\n",
    "        res = classifier.forward(paper_rep)\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            test_res += [(bi == ai).int().tolist()]\n",
    "    test_ndcg = [ndcg_at_k(resi, len(resi)) for resi in test_res]\n",
    "    print('Best Test NDCG: %.4f' % np.average(test_ndcg))\n",
    "    test_mrr = mean_reciprocal_rank(test_res)\n",
    "    print('Best Test MRR:  %.4f' % np.average(test_mrr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
